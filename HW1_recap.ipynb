{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7448c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7395042",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c71821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Read data\n",
    "#df = pd.read_csv('data/amazon_reviews_us_Kitchen_v1_00.tsv', sep='\\t', on_bad_lines='skip')\n",
    "df = pd.read_csv('https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz', sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26607b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Beautiful.  Looks great on counter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I personally have 5 days sets and have also bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Fabulous and worth every penny. Used for clean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>A must if you love garlic on tomato marinara s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Worth every penny! Buy one now and be a pizza ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ratings                                            reviews\n",
       "0      5.0                Beautiful.  Looks great on counter.\n",
       "1      5.0  I personally have 5 days sets and have also bo...\n",
       "2      5.0  Fabulous and worth every penny. Used for clean...\n",
       "3      5.0  A must if you love garlic on tomato marinara s...\n",
       "4      5.0  Worth every penny! Buy one now and be a pizza ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['star_rating', 'review_body']].rename(columns={'star_rating':'ratings', 'review_body':'reviews'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6d73b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">reviews</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratings</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>426870</td>\n",
       "      <td>419692</td>\n",
       "      <td>Too small</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>241939</td>\n",
       "      <td>239368</td>\n",
       "      <td>Too small</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>349539</td>\n",
       "      <td>342257</td>\n",
       "      <td>ok</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>731701</td>\n",
       "      <td>703142</td>\n",
       "      <td>good</td>\n",
       "      <td>1163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>3124595</td>\n",
       "      <td>2876141</td>\n",
       "      <td>Great</td>\n",
       "      <td>5604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviews                          \n",
       "           count   unique        top  freq\n",
       "ratings                                   \n",
       "1.0       426870   419692  Too small   110\n",
       "2.0       241939   239368  Too small   105\n",
       "3.0       349539   342257         ok   550\n",
       "4.0       731701   703142       good  1163\n",
       "5.0      3124595  2876141      Great  5604"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('ratings').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ce4eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 1. 4. 2.]\n",
      "                                             reviews  label\n",
      "0                Beautiful.  Looks great on counter.      1\n",
      "1  I personally have 5 days sets and have also bo...      1\n",
      "2  Fabulous and worth every penny. Used for clean...      1\n",
      "3  A must if you love garlic on tomato marinara s...      1\n",
      "4  Worth every penny! Buy one now and be a pizza ...      1\n"
     ]
    }
   ],
   "source": [
    "# Create binary labels: ratings > 3 --> 1, ratings < 3 --> 0, ratings == 3 --> discard\n",
    "# Discard ratings equal to 3\n",
    "df = df[(df['ratings']>3)|(df['ratings']<3)]\n",
    "print(df['ratings'].unique())\n",
    "# Map ratings > 3 to 1, and ratings < 3 to 0\n",
    "df['label'] = df['ratings'].map(lambda x: 1 if x > 3 else 0)\n",
    "df.drop('ratings', axis=1, inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd73c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   reviews  label\n",
      "3490548  very good product and met every expectations. ...      1\n",
      "2590127  These are pretty and fun.  The only problem I ...      1\n",
      "3283260  LOVE THIS!!! It is sturdy, elegant and is easy...      1\n",
      "2859885  I didn't have a pot for our town house, and ki...      1\n",
      "3201281  I love it, it looks so nice under my Keurig an...      1\n",
      "                                                   reviews  label\n",
      "2655570  We liked the looks of this timer, and the fact...      0\n",
      "3261497  i bought a swell bottle. LOVED it. kept hot th...      0\n",
      "763143             Worse thing I have very bought on line.      0\n",
      "549574             Product did not meet my need. Too small      0\n",
      "4405080  We were happy enough with this toaster -- for ...      0\n",
      "                                             reviews  label\n",
      "0  We love veggies and salads, but I've always ha...      1\n",
      "1  I recently purchased 2 Grazia Premium silicone...      1\n",
      "2                                          not exact      0\n",
      "3  I love this thing.  It takes some effort to gr...      1\n",
      "4  Ordered 3 of these for a wedding reception - 1...      1\n"
     ]
    }
   ],
   "source": [
    "# Downsize the dataframe with 100,000 positive reviews and 100,000 negative ones.\n",
    "df_p = df[df['label']==1].sample(n=100000, random_state=1)\n",
    "df_n = df[df['label']==0].sample(n=100000, random_state=1)\n",
    "print(df_p.head())\n",
    "print(df_n.head())\n",
    "df = pd.concat([df_p, df_n]).sample(frac=1).reset_index(drop=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c47fd",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56359905",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['reviews'].fillna('').tolist(), df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ada913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reviews to lower case\n",
    "X = list(map(lambda x: str(x).lower(), X))\n",
    "# remove HTML and URLs from reviews\n",
    "X = list(map(lambda x: re.sub('<.*>', '', x), X))\n",
    "X = list(map(lambda x: re.sub(r'https?://\\S+', '', x), X))\n",
    "# remove non-alphabetical characters\n",
    "X = list(map(lambda x: re.sub('[^a-z ]', '', x), X))\n",
    "# remove extra spaces\n",
    "X = list(map(lambda x: re.sub(' +', ' ', x), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58d1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand contractions\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "def decontraction(s):\n",
    "    for word in s.split(' '):\n",
    "        if word in contractions.keys():\n",
    "            s = re.sub(word, contractions[word], s)\n",
    "    return s\n",
    "X = list(map(decontraction, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d4640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stopWords =set(stopwords.words('english'))\n",
    "def rmstopWords(s):\n",
    "    wordlist = s.split(' ')\n",
    "    newlist = []\n",
    "    for word in wordlist:\n",
    "        if word not in stopWords:\n",
    "            newlist.append(word)\n",
    "    s = ' '.join(newlist)\n",
    "    return s\n",
    "\n",
    "X = list(map(rmstopWords, X))\n",
    "\n",
    "# perform lemmatization\n",
    "wnl = WordNetLemmatizer()\n",
    "X = list(map(lambda x: ' '.join(map(wnl.lemmatize, x.split(' '))), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de1b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the downsized dataset into 80% training dataset and 20% testing dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571c879",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b2c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(X_train)\n",
    "tfidf_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6999f43",
   "metadata": {},
   "source": [
    "## 4. Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b83e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training dataset: 91.4%\n",
      "The precision of training dataset: 93.5%\n",
      "The recall of training dataset: 89.0%\n",
      "The fscore of training dataset: 91.2%\n",
      "\n",
      "The accuracy of testing dataset: 84.8%\n",
      "The precision of testing dataset: 87.0%\n",
      "The recall of testing dataset: 81.8%\n",
      "The fscore of testing dataset: 84.3%\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(random_state=1)\n",
    "perceptron.fit(tfidf, y_train)\n",
    "y_train_predict, y_test_predict = perceptron.predict(tfidf), perceptron.predict(tfidf_test)\n",
    "\n",
    "# report accuracy, precision, recall, and f1-score on both the training and testing split\n",
    "train_stats = precision_recall_fscore_support(y_train, y_train_predict, average='binary')\n",
    "precision_train, recall_train, fscore_train = train_stats[0], train_stats[1], train_stats[2]\n",
    "\n",
    "test_stats = precision_recall_fscore_support(y_test, y_test_predict, average='binary')\n",
    "precision_test, recall_test, fscore_test = test_stats[0], test_stats[1], test_stats[2]\n",
    "\n",
    "print('The accuracy of training dataset: {:2.1%}'.format(perceptron.score(tfidf, y_train)))\n",
    "print('The precision of training dataset: {:2.1%}'.format(precision_train))\n",
    "print('The recall of training dataset: {:2.1%}'.format(recall_train))\n",
    "print('The fscore of training dataset: {:2.1%}\\n'.format(fscore_train))\n",
    "\n",
    "print('The accuracy of testing dataset: {:2.1%}'.format(perceptron.score(tfidf_test, y_test)))\n",
    "print('The precision of testing dataset: {:2.1%}'.format(precision_test))\n",
    "print('The recall of testing dataset: {:2.1%}'.format(recall_test))\n",
    "print('The fscore of testing dataset: {:2.1%}'.format(fscore_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c69ca3",
   "metadata": {},
   "source": [
    "## 5. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9d4874",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training dataset: 93.9%\n",
      "The precision of training dataset: 94.0%\n",
      "The recall of training dataset: 93.8%\n",
      "The fscore of training dataset: 93.9%\n",
      "\n",
      "The accuracy of testing dataset: 89.2%\n",
      "The precision of testing dataset: 89.0%\n",
      "The recall of testing dataset: 89.4%\n",
      "The fscore of testing dataset: 89.2%\n"
     ]
    }
   ],
   "source": [
    "# refer to https://stackoverflow.com/questions/52008548/python-running-into-x-test-y-test-fit-errors\n",
    "# for why with_mean should be set to False\n",
    "svm = LinearSVC(random_state=1)\n",
    "svm.fit(tfidf, y_train)\n",
    "\n",
    "y_train_predict, y_test_predict = svm.predict(tfidf), svm.predict(tfidf_test)\n",
    "\n",
    "# report accuracy, precision, recall, and f1-score on both the training and testing split\n",
    "train_stats = precision_recall_fscore_support(y_train, y_train_predict, average='binary')\n",
    "precision_train, recall_train, fscore_train = train_stats[0], train_stats[1], train_stats[2]\n",
    "\n",
    "test_stats = precision_recall_fscore_support(y_test, y_test_predict, average='binary')\n",
    "precision_test, recall_test, fscore_test = test_stats[0], test_stats[1], test_stats[2]\n",
    "\n",
    "print('The accuracy of training dataset: {:2.1%}'.format(svm.score(tfidf, y_train)))\n",
    "print('The precision of training dataset: {:2.1%}'.format(precision_train))\n",
    "print('The recall of training dataset: {:2.1%}'.format(recall_train))\n",
    "print('The fscore of training dataset: {:2.1%}\\n'.format(fscore_train))\n",
    "\n",
    "print('The accuracy of testing dataset: {:2.1%}'.format(svm.score(tfidf_test, y_test)))\n",
    "print('The precision of testing dataset: {:2.1%}'.format(precision_test))\n",
    "print('The recall of testing dataset: {:2.1%}'.format(recall_test))\n",
    "print('The fscore of testing dataset: {:2.1%}'.format(fscore_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ab338",
   "metadata": {},
   "source": [
    "## 6. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "400bee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training dataset: 91.1%\n",
      "The precision of training dataset: 91.4%\n",
      "The recall of training dataset: 90.8%\n",
      "The fscore of training dataset: 91.1%\n",
      "\n",
      "The accuracy of testing dataset: 89.6%\n",
      "The precision of testing dataset: 89.7%\n",
      "The recall of testing dataset: 89.5%\n",
      "The fscore of testing dataset: 89.6%\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(random_state=1, max_iter=200)\n",
    "logistic.fit(tfidf, y_train)\n",
    "\n",
    "y_train_predict, y_test_predict = logistic.predict(tfidf), logistic.predict(tfidf_test)\n",
    "\n",
    "# report accuracy, precision, recall, and f1-score on both the training and testing split\n",
    "train_stats = precision_recall_fscore_support(y_train, y_train_predict, average='binary')\n",
    "precision_train, recall_train, fscore_train = train_stats[0], train_stats[1], train_stats[2]\n",
    "\n",
    "test_stats = precision_recall_fscore_support(y_test, y_test_predict, average='binary')\n",
    "precision_test, recall_test, fscore_test = test_stats[0], test_stats[1], test_stats[2]\n",
    "\n",
    "print('The accuracy of training dataset: {:2.1%}'.format(logistic.score(tfidf, y_train)))\n",
    "print('The precision of training dataset: {:2.1%}'.format(precision_train))\n",
    "print('The recall of training dataset: {:2.1%}'.format(recall_train))\n",
    "print('The fscore of training dataset: {:2.1%}\\n'.format(fscore_train))\n",
    "\n",
    "print('The accuracy of testing dataset: {:2.1%}'.format(logistic.score(tfidf_test, y_test)))\n",
    "print('The precision of testing dataset: {:2.1%}'.format(precision_test))\n",
    "print('The recall of testing dataset: {:2.1%}'.format(recall_test))\n",
    "print('The fscore of testing dataset: {:2.1%}'.format(fscore_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0931666",
   "metadata": {},
   "source": [
    "## 7. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd6f5026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training dataset: 89.0%\n",
      "The precision of training dataset: 89.8%\n",
      "The recall of training dataset: 88.0%\n",
      "The fscore of training dataset: 88.9%\n",
      "\n",
      "The accuracy of testing dataset: 87.2%\n",
      "The precision of testing dataset: 88.0%\n",
      "The recall of testing dataset: 86.0%\n",
      "The fscore of testing dataset: 87.0%\n"
     ]
    }
   ],
   "source": [
    "multiNB = MultinomialNB()\n",
    "multiNB.fit(tfidf, y_train)\n",
    "y_train_predict, y_test_predict = multiNB.predict(tfidf), multiNB.predict(tfidf_test)\n",
    "\n",
    "# report accuracy, precision, recall, and f1-score on both the training and testing split\n",
    "train_stats = precision_recall_fscore_support(y_train, y_train_predict, average='binary')\n",
    "precision_train, recall_train, fscore_train = train_stats[0], train_stats[1], train_stats[2]\n",
    "\n",
    "test_stats = precision_recall_fscore_support(y_test, y_test_predict, average='binary')\n",
    "precision_test, recall_test, fscore_test = test_stats[0], test_stats[1], test_stats[2]\n",
    "\n",
    "print('The accuracy of training dataset: {:2.1%}'.format(multiNB.score(tfidf, y_train)))\n",
    "print('The precision of training dataset: {:2.1%}'.format(precision_train))\n",
    "print('The recall of training dataset: {:2.1%}'.format(recall_train))\n",
    "print('The fscore of training dataset: {:2.1%}\\n'.format(fscore_train))\n",
    "\n",
    "print('The accuracy of testing dataset: {:2.1%}'.format(multiNB.score(tfidf_test, y_test)))\n",
    "print('The precision of testing dataset: {:2.1%}'.format(precision_test))\n",
    "print('The recall of testing dataset: {:2.1%}'.format(recall_test))\n",
    "print('The fscore of testing dataset: {:2.1%}'.format(fscore_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1576922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 90.12 s\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print('time elapsed: {:.2f} s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639b9db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
